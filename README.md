# AutomaticAnswerEvaluation

Evaluation is a major driving force in advancing the state of the art in language technologies. The methods for automatically assessing the quality of machine output is the preferred method provided this automatic methods are validated against the human judgments.There has been recent developments in the automatic evaluation of machine translation and document summarization. We present an automatic technique for evaluating answers to complex questions based on the co-occurrences in the subject and the actions between machine output and human generated answers.The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003, TREC 2004, and TREC 2005 QA tracks indicate that rankings produced by our metric correlate highly with official rankings between machine output and a human-generated answer key
